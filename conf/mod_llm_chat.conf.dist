[worldserver]

###################################################################################################
# LLM CHAT SYSTEM
#
# These settings control the LLM Chat System
#
#    LLM.Enable
#        Description: Enables LLM Chat System
#        Default:     True  - (Enabled)
#                     False - (Disabled)

LLM.Enable = true

#
#    LLM.Provider
#        Description: Which LLM provider to use
#        1 - Ollama
#        2 - LM Studio
#        Default: 1 (Ollama)
#

LLM.Provider = 1

#
#    LLM.Ollama.Endpoint
#        Description: Endpoint URL for Ollama API
#        Default: "http://localhost:11435/api/generate"
#

LLM.Ollama.Endpoint = "http://localhost:11435/api/generate"

#
#    LLM.Ollama.Model
#        Description: Model to use with Ollama
#        Default: "llama3.2:3b"
#

LLM.Ollama.Model = "llama3.2:3b"

#
#    LLM.ChatRange
#        Description: Range in yards for AI chat responses to be visible
#        Default: 25.0
#

LLM.ChatRange = 25.0

#
#    LLM.ResponsePrefix
#        Description: Prefix for AI responses in chat
#        Default: "[AI] "
#

LLM.ResponsePrefix = "[AI] "

#
#    LLM.LogLevel
#        Description: Level of logging detail
#        0 - Disabled
#        1 - Errors Only
#        2 - Basic
#        3 - Detailed
#        Default: 2
#

LLM.LogLevel = 2

#
################################################################################################### 