[worldserver]

###################################################################################################
# LLM CHAT SYSTEM
###################################################################################################

#    LLMChat.Enable
#        Description: Enable the LLM Chat module
#        Default:     1  - (Enabled)
#                     0  - (Disabled)

LLMChat.Enable = 1

#    LLMChat.Announce
#        Description: Announce the module on player login
#        Default:     1  - (Enabled)
#                     0  - (Disabled)

LLMChat.Announce = 1

#    LLMChat.Provider
#        Description: Which LLM provider to use
#        Default:     1  - (Ollama)
#                     2  - (LM Studio)

LLMChat.Provider = 1

#    LLMChat.Ollama.Endpoint
#        Description: Endpoint URL for Ollama API
#        Default:    "http://localhost:11434/api/generate"

LLMChat.Ollama.Endpoint = "http://localhost:11434/api/generate"

#    LLMChat.Ollama.Model
#        Description: Model to use with Ollama
#        Default:    "llama3.2:3b"

LLMChat.Ollama.Model = "llama3.2:3b"

#    LLMChat.ChatRange
#        Description: Range in yards for AI chat responses to be visible
#        Default:     25.0

LLMChat.ChatRange = 25.0

#    LLMChat.ResponsePrefix
#        Description: Prefix for AI responses in chat
#        Default:    "[AI] "

LLMChat.ResponsePrefix = "[AI] "

#    LLMChat.LogLevel
#        Description: Level of logging detail
#        Default:     2  - (Basic)
#                     0  - (Disabled)
#                     1  - (Errors Only)
#                     3  - (Detailed)

LLMChat.LogLevel = 2

################################################################################################### 