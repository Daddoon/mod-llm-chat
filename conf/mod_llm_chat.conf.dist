[worldserver]

###################################################################################################
# LLM CHAT SYSTEM
###################################################################################################

#    LLMChat.Enable
#        Description: Enable the LLM Chat module
#        Default:     1  - (Enabled)
#                     0  - (Disabled)

LLMChat.Enable = 1

#    LLMChat.Announce
#        Description: Announce the module on player login
#        Default:     0  - (Disabled to maintain immersion)
#                     1  - (Enabled)

LLMChat.Announce = 1

#    LLMChat.Provider
#        Description: Which LLM provider to use
#        Default:     1  - (Ollama)
#                     2  - (LM Studio)

LLMChat.Provider = 1

#    LLMChat.Ollama.Endpoint
#        Description: Endpoint URL for Ollama API
#        Default:    "http://localhost:11434/api/generate"

LLMChat.Ollama.Endpoint = "http://localhost:11434/api/generate"

#    LLMChat.Ollama.Model
#        Description: Model to use with Ollama
#        Default:    "krith/meta-llama-3.2-1b-instruct-uncensored:IQ3_M"

LLMChat.Ollama.Model = "krith/meta-llama-3.2-1b-instruct-uncensored:IQ3_M"

#    LLMChat.ChatRange
#        Description: Range in yards for chat responses to be visible
#        Default:     25.0

LLMChat.ChatRange = 25.0

#    LLMChat.ResponsePrefix
#        Description: Prefix for responses (empty for natural chat)
#        Default:    ""  - (No prefix for natural chat)
#                    "[AI] "  - (Example prefix)

LLMChat.ResponsePrefix = ""

#    LLMChat.LogLevel
#        Description: Level of logging detail
#        Default:     3  - (Detailed)
#                     0  - (Disabled)
#                     1  - (Errors Only)
#                     2  - (Basic)

LLMChat.LogLevel = 3

#    LLMChat.MaxResponsesPerMessage
#        Description: Maximum number of bots that can respond to a single message
#        Default:     2  - (Two bots maximum)
#                     0  - (Disabled)
#                     1-10  - (Number of responding bots)

LLMChat.MaxResponsesPerMessage = 3

#    LLMChat.MaxConversationRounds
#        Description: Maximum number of back-and-forth exchanges in a conversation
#        Default:     3  - (Three rounds of responses)
#                     0  - (Disabled)
#                     1-10  - (Number of conversation rounds)

LLMChat.MaxConversationRounds = 3

#    LLMChat.ResponseChance
#        Description: Percentage chance (0-100) that an eligible bot will respond
#        Default:     50  - (50% chance to respond)
#                     0  - (Never respond)
#                     100  - (Always respond)

LLMChat.ResponseChance = 75

#    LLMChat.LLM.Temperature
#        Description: Controls response creativity (0.1-2.0)
#        Default:     1.2  - (More creative responses)
#                     0.1  - (More focused/deterministic)
#                     2.0  - (Maximum creativity)

LLMChat.LLM.Temperature = 1.5

#    LLMChat.LLM.TopP
#        Description: Controls response diversity (0.1-1.0)
#        Default:     0.95  - (High diversity)
#                     0.1   - (More focused)
#                     1.0   - (Maximum diversity)

LLMChat.LLM.TopP = 0.95

#    LLMChat.LLM.NumPredict
#        Description: Maximum response length in tokens
#        Default:     1024  - (Standard length)
#                     256   - (Short responses)
#                     2048  - (Long responses)

LLMChat.LLM.NumPredict = 2048

#    LLMChat.LLM.ContextSize
#        Description: Size of context window in tokens
#        Default:     4096  - (Standard context)
#                     2048  - (Smaller context)
#                     8192  - (Larger context)

LLMChat.LLM.ContextSize = 4096

#    LLMChat.LLM.RepeatPenalty
#        Description: Penalty for repeating words (1.0-2.0)
#        Default:     1.3  - (Higher penalty for more variation)
#                     1.0  - (No penalty)
#                     2.0  - (Maximum penalty)

LLMChat.LLM.RepeatPenalty = 1.3

#    LLMChat.PersonalityFile
#        Description: Path to personality definitions JSON file
#        Default:    "mod_llm_chat/conf/personalities.json"
#        Contains personality definitions, emotion types, and chat styles
#        Edit this file to add or modify AI personalities

LLMChat.PersonalityFile = "mod_llm_chat/conf/personalities.json"

################################################################################################### 