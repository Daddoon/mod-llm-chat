# LLM Chat Module Configuration

###################################################################################################
# SECTION 1: Core Settings
###################################################################################################

#
#    LLMChat.Enable
#        Description: Enable or disable the LLM Chat module
#        Default:     1 - Enabled
#        Note:        Make sure Ollama service is running before enabling
#

LLMChat.Enable = 1

#
#    LLMChat.LogLevel
#        Description: Logging level for the module
#        Default:     2 - Detailed logging
#        Values:      0 = Disabled (no logging)
#                    1 = Minimal (errors only)
#                    2 = Detailed (errors + events + chat interactions)
#                    3 = Debug (all above + technical details)
#        Note:        Higher levels include all lower level logs (except 0 which disables all logs)
#

LLMChat.LogLevel = 3

#
#    LLMChat.Announce
#        Description: Announce module to players on login
#        Default:     0 - Disabled
#

LLMChat.Announce = 1

###################################################################################################
# SECTION 2: LLM Provider Settings
###################################################################################################

#
#    LLMChat.Endpoint
#        Description: API endpoint URL for the LLM service
#        Default:     "http://localhost:11434/api/generate"
#        Note:        For Ollama, use the default. For other providers, use their API endpoint.
#
#        Available Endpoints (commented examples):
#
#        Local Deployments (Free):
#            - "http://localhost:11434/api/chat"    (Ollama - Default)
#                Website: https://ollama.ai
#                Note: Free, self-hosted, runs locally
#
#            - "http://localhost:1234/v1/chat/completions"    (LM Studio)
#                Website: https://lmstudio.ai
#                Note: Free, self-hosted local GUI for running models
#
#            - "http://localhost:8080/v1/completions"    (Text Generation WebUI)
#                Website: https://github.com/oobabooga/text-generation-webui
#                Note: Free, self-hosted web interface
#
#        Cloud Providers (Paid with Free Tiers):
#            - "https://api.openai.com/v1/chat/completions"    (OpenAI)
#                Website: https://openai.com
#                Pricing: Varies by model ($0.0005-$0.03/1K tokens)
#                Free Tier: $5 credit for new accounts
#                Note: Requires API key
#
#            - "https://api.anthropic.com/v1/messages"    (Anthropic)
#                Website: https://anthropic.com
#                Pricing: Varies by model ($0.0005-$0.15/1K tokens)
#                Free Tier: Limited free credits for new users
#                Note: Requires API key
#
#            - "https://api.together.xyz/inference"    (Together AI)
#                Website: https://www.together.ai
#                Pricing: Pay-as-you-go ($0.0002-$0.0015/1K tokens)
#                Free Tier: Yes - $25 in credits, includes most open source models
#                Note: Large selection of free and paid models
#
#            - "https://api.groq.com/openai/v1/chat/completions"    (Groq)
#                Website: https://groq.com
#                Pricing: $0.0001-$0.0003/1K tokens
#                Free Tier: Yes - includes LLaMA2 70B and Mixtral 8x7B models
#                Note: Known for extremely fast inference speeds (sub-100ms)
#                      Free access to several open source models
#                      No credit card required for free tier
#
#            - "https://api.deepinfra.com/v1/openai/chat/completions"    (DeepInfra)
#                Website: https://deepinfra.com
#                Pricing: Varies by model ($0.0001-$0.001/1K tokens)
#                Free Tier: Yes - $10 in credits, includes Mixtral and other models
#                Note: Offers many open source models with free inference
#
#            - "https://api.perplexity.ai/chat/completions"    (Perplexity)
#                Website: https://www.perplexity.ai
#                Free Tier: Yes - includes pplx-7b-online and pplx-70b-online
#                Note: 1M tokens per month free
#                      No credit card required for free tier
#
#            - "https://api.fireworks.ai/inference/v1/chat/completions"    (Fireworks)
#                Website: https://fireworks.ai
#                Free Tier: Yes - $10 in credits
#                Note: Access to Mixtral, Llama, and other models
#                      Competitive pricing for paid usage
#
#            - "https://api.mistral.ai/v1/chat/completions"    (Mistral)
#                Website: https://mistral.ai
#                Free Tier: Yes - includes mistral-tiny and mistral-small
#                Pricing: From â‚¬0.14/1M tokens
#                Note: Created by Mistral AI team
#
#            - "https://api.anyscale.ai/v1/chat/completions"    (Anyscale)
#                Website: https://www.anyscale.ai
#                Free Tier: Yes - includes Llama 2 and Mistral models
#                Note: 1M tokens per month free
#                      No credit card required
#
#            - "https://api.claude.ai/v1/messages"    (Claude.ai)
#                Website: https://claude.ai
#                Free Tier: Yes - web interface only (no API)
#                Note: Free access to Claude 3 Sonnet
#                      Available in supported countries
#
#        Note: Pricing information is approximate and subject to change
#              Many providers offer additional free credits for startups/researchers
#              Some services have usage quotas or rate limits on free tiers
#              Check provider websites for current offers and availability
#

LLMChat.Endpoint = "http://localhost:11434/api/generate"

#
#    LLMChat.Model
#        Description: Model to use for chat responses
#        Default:     "socialnetwooky/llama3.2-abliterated:1b_q8"
#        Note:        For Ollama, run: ollama pull socialnetwooky/llama3.2-abliterated:1b_q8
#                    For other providers, use their model identifier
#
#        Available Models (commented examples):
#        Ollama Models:
#            - "socialnetwooky/llama3.2-abliterated:1b_q8" (default, recommended)
#            - "llama2:7b-chat"      (Larger, more capable)
#            - "llama2:13b-chat"     (Even larger, better responses)
#            - "mistral:7b"          (Good alternative to llama2)
#            - "mixtral:8x7b"        (Very powerful mixture of experts model)
#            - "neural-chat:7b"      (Optimized for chat)
#            - "starling-lm:7b"      (Good for roleplay)
#            - "codellama:7b"        (Code-focused variant)
#            - "dolphin-phi:2.7b"    (Smaller but capable)
#            - "phi-2:2.7b"          (Microsoft's small model)
#
#        OpenAI Models (requires API key):
#            - "gpt-3.5-turbo"       (Fast, cost-effective)
#            - "gpt-4"               (Most capable)
#            - "gpt-4-turbo-preview" (Latest GPT-4)
#
#        Anthropic Models (requires API key):
#            - "claude-3-opus"       (Most capable)
#            - "claude-3-sonnet"     (Balanced performance)
#            - "claude-3-haiku"      (Fast, efficient)
#            - "claude-2.1"          (Previous generation)
#
#        Free/Open Source Models (via various providers):
#            - "openchat:7b"         (Open source chat model)
#            - "stable-beluga:7b"    (Stable diffusion team's model)
#            - "vicuna:7b"           (Berkeley's model)
#            - "wizardlm:7b"         (Specialized in complex tasks)
#            - "zephyr:7b"           (Balanced performance)
#            - "nous-hermes:7b"      (Well-rounded performer)
#            - "orca-2:13b"          (Microsoft research model)
#            - "yi:6b"               (Efficient smaller model)
#
#        Note: Larger models (13B+) require more system resources
#              Some models may require specific provider endpoints
#              Always check licensing and usage requirements
#

LLMChat.Model = "socialnetwooky/llama3.2-abliterated:1b_q8"

#
#    LLMChat.ApiKey
#        Description: API key for authentication (if required)
#        Default:     ""
#        Note:        Required for most cloud providers. Leave empty for Ollama.
#

LLMChat.ApiKey = ""

#
#    LLMChat.ApiSecret
#        Description: API secret for authentication (if required)
#        Default:     ""
#        Note:        Required for some providers. Leave empty if not needed.
#

LLMChat.ApiSecret = ""

###################################################################################################
# SECTION 3: Chat Behavior Settings
###################################################################################################

#
#    LLMChat.ChatRange
#        Description: Maximum range in yards for chat responses
#        Default:     30.0
#

LLMChat.ChatRange = 30.0

#
#    LLMChat.ResponsePrefix
#        Description: Prefix to add to AI responses (empty for none)
#        Default:     ""
#

LLMChat.ResponsePrefix = ""

#
#    LLMChat.MaxResponsesPerMessage
#        Description: Maximum number of AI responses per player message
#        Default:     3
#

LLMChat.MaxResponsesPerMessage = 3

###################################################################################################
# SECTION 4: Queue Settings
###################################################################################################

#
#    LLMChat.Queue.MaxResponses
#        Description: Maximum number of responses to queue per interaction
#        Default:     3
#

LLMChat.Queue.MaxResponses = 3

#
#    LLMChat.Queue.GlobalCooldown
#        Description: Global cooldown between responses in milliseconds
#        Default:     1000
#

LLMChat.Queue.GlobalCooldown = 1000

#
#    LLMChat.Queue.BotCooldown
#        Description: Per-bot cooldown between responses in milliseconds
#        Default:     5000
#

LLMChat.Queue.BotCooldown = 5000

###################################################################################################
# SECTION 5: Combat Settings
###################################################################################################

#
#    LLMChat.Combat.Enable
#        Description: Enable or disable combat-related chat responses
#        Default:     0
#

LLMChat.Combat.Enable = 0

#
#    LLMChat.Combat.MaxPlayers
#        Description: Maximum number of players in combat for responses
#        Default:     5
#

LLMChat.Combat.MaxPlayers = 5

#
#    LLMChat.Combat.MinPlayers
#        Description: Minimum number of players in combat for responses
#        Default:     1
#

LLMChat.Combat.MinPlayers = 1

#
#    LLMChat.Combat.RequireTargetInCombat
#        Description: Whether the target must be in combat for responses
#        Default:     1
#

LLMChat.Combat.RequireTargetInCombat = 1

