[worldserver]

###################################################################################################
#  MOD LLM CHAT
#
#  Enable LLM Chat Module
#  Default: 1 (enabled)
#         0 (disabled)
#

ModLLMChat.Enable = 1

#
#  Ollama Configuration
#
#  Default host and port for Ollama service
#

ModLLMChat.Ollama.Host = "http://localhost"
ModLLMChat.Ollama.Port = 11434
ModLLMChat.Ollama.Model = "yi:3b"

#
#  Chat Settings
#
#  Configure chat behavior
#

ModLLMChat.MaxTokens = 2048
ModLLMChat.Temperature = 0.7
ModLLMChat.TopP = 0.9 